{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ca8ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# --- Config ---\n",
    "CHUNK_SIZE = 750\n",
    "CHUNK_OVERLAP = 100\n",
    "DATA_DIR = Path(\"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/raw_data\")\n",
    "OUTPUT_DIR = Path(\"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks\")\n",
    "\n",
    "# --- Helpers ---\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(len(text), start + chunk_size)\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Replace tabs, newlines, and carriage returns with single spaces\n",
    "    cleaned = re.sub(r'[\\t\\n\\r]+', ' ', text)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        reader = PdfReader(str(pdf_path))\n",
    "        text = \"\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "        return clean_text(text)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to read {pdf_path.name}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def process_author_folder(author_path: Path):\n",
    "    author = author_path.name\n",
    "    output_path = OUTPUT_DIR / f\"{author}.json\"\n",
    "    results = []\n",
    "\n",
    "    for pdf_file in tqdm(author_path.glob(\"*.pdf\"), desc=f\"Chunking {author}\"):\n",
    "        text = extract_text_from_pdf(pdf_file)\n",
    "        if not text.strip():\n",
    "            continue\n",
    "\n",
    "        chunks = chunk_text(text)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            results.append({\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"text\": chunk,\n",
    "                \"metadata\": {\n",
    "                    \"filename\": pdf_file.name,\n",
    "                    \"author\": author\n",
    "                }\n",
    "            })\n",
    "\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Saved {len(results)} chunks to {output_path}\")\n",
    "\n",
    "# Example usage:\n",
    "# from chunk_author_to_json import process_author_folder, DATA_DIR\n",
    "# process_author_folder(DATA_DIR / \"Aristotle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e18d55f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorfolders = {\n",
    "    1:\"Byung-Chul Han\",\n",
    "    2:\"Carl Schmitt\",\n",
    "    3:\"Deleuze and Guattari\",\n",
    "    4:\"Ernst Juenger\",\n",
    "    5:\"Jacques Ellul\",\n",
    "    6:\"Jean Baudrillard\",\n",
    "    7:\"Lewis Mumford\",\n",
    "    8:\"Marshall McLuhan\",\n",
    "    9:\"Nick Land\",\n",
    "    10:\"Paul Virilio\",\n",
    "    11:\"Peter Sloterdijk\",\n",
    "    12:\"Spengler\",\n",
    "    13:\"Vilem Flusser\",\n",
    "    14:\"Walter Benjamin\",\n",
    "    15:\"Walter Ong\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4289d0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deleuze and Guattari'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorfolders[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15998ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Byung-Chul Han: 4it [00:09,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 798 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Byung-Chul Han.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_author_folder(DATA_DIR / authorfolders[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a854d5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Carl Schmitt: 3it [00:02,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 129 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Carl Schmitt.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Deleuze and Guattari: 1it [00:22, 22.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 1739 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Deleuze and Guattari.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Ernst Juenger: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 0 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Ernst Juenger.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Jacques Ellul: 3it [00:49, 16.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 1777 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Jacques Ellul.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Jean Baudrillard: 5it [00:08,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 1822 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Jean Baudrillard.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_author_folder(DATA_DIR / authorfolders[2])\n",
    "process_author_folder(DATA_DIR / authorfolders[3])\n",
    "process_author_folder(DATA_DIR / authorfolders[4])\n",
    "process_author_folder(DATA_DIR / authorfolders[5])\n",
    "process_author_folder(DATA_DIR / authorfolders[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cb709ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Lewis Mumford: 6it [01:34, 15.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 5790 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Lewis Mumford.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Marshall McLuhan: 0it [00:00, ?it/s]incorrect startxref pointer(3)\n",
      "parsing for Object Streams\n",
      "Chunking Marshall McLuhan: 5it [00:17,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 3179 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Marshall McLuhan.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Nick Land: 5it [00:38,  7.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 3724 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Nick Land.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Paul Virilio: 7it [00:08,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 1199 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Paul Virilio.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Peter Sloterdijk: 0it [00:00, ?it/s]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Chunking Peter Sloterdijk: 9it [00:06,  1.66it/s]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 96 0 (offset 0)\n",
      "Ignoring wrong pointing object 98 0 (offset 0)\n",
      "Chunking Peter Sloterdijk: 10it [00:08,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 948 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Peter Sloterdijk.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_author_folder(DATA_DIR / authorfolders[7])\n",
    "process_author_folder(DATA_DIR / authorfolders[8])\n",
    "process_author_folder(DATA_DIR / authorfolders[9])\n",
    "process_author_folder(DATA_DIR / authorfolders[10])\n",
    "process_author_folder(DATA_DIR / authorfolders[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fce31ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Spengler: 1it [00:48, 48.28s/it]Overwriting cache for 0 166\n",
      "Chunking Spengler: 5it [01:11, 14.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 5065 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Spengler.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Vilem Flusser: 4it [00:24,  6.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 2376 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Vilem Flusser.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Walter Benjamin: 2it [00:24, 12.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 2204 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Walter Benjamin.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Walter Ong: 5it [00:08,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 1033 chunks to /Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks/Walter Ong.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_author_folder(DATA_DIR / authorfolders[12])\n",
    "process_author_folder(DATA_DIR / authorfolders[13])\n",
    "process_author_folder(DATA_DIR / authorfolders[14])\n",
    "process_author_folder(DATA_DIR / authorfolders[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(authorfolders)):\n",
    "#     process_author_folder(DATA_DIR / authorfolders[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848b8fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mason/opt/anaconda3/envs/psai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- Config ---\n",
    "CHUNKS_DIR = Path(\"/Users/mason/Desktop/Technical_Projects/PYTHON_Projects/ResearchAI/chunks\")\n",
    "QDRANT_HOST = \"localhost\"\n",
    "QDRANT_PORT = 6333\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# --- Init ---\n",
    "model = SentenceTransformer(EMBED_MODEL)\n",
    "client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)\n",
    "\n",
    "def batch_upsert(collection_name, points, batch_size=BATCH_SIZE):\n",
    "    for i in range(0, len(points), batch_size):\n",
    "        batch = points[i:i + batch_size]\n",
    "        client.upsert(collection_name=collection_name, points=batch)\n",
    "\n",
    "def process_json_file(json_path: Path):\n",
    "    collection_name = json_path.stem.lower().replace(\" \", \"_\")\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if not data:\n",
    "        print(f\"⚠️ Skipping empty file: {json_path.name}\")\n",
    "        return\n",
    "\n",
    "    # Ensure collection exists\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(\n",
    "            size=model.get_sentence_embedding_dimension(),\n",
    "            distance=Distance.COSINE,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    points = []\n",
    "    for item in tqdm(data, desc=f\"Embedding {json_path.name}\"):\n",
    "        vector = model.encode(item[\"text\"]).tolist()\n",
    "        points.append(PointStruct(\n",
    "            id=item[\"id\"],\n",
    "            vector=vector,\n",
    "            payload={\n",
    "                \"text\": item[\"text\"],\n",
    "                **item[\"metadata\"]\n",
    "            }\n",
    "        ))\n",
    "\n",
    "    batch_upsert(collection_name, points)\n",
    "    print(f\"✅ Uploaded {len(points)} points to collection: {collection_name}\")\n",
    "\n",
    "# --- Run all .json files ---\n",
    "def upload_all_json_chunks():\n",
    "    existing_collections = [c.name for c in client.get_collections().collections]\n",
    "\n",
    "    for json_file in CHUNKS_DIR.glob(\"*.json\"):\n",
    "        collection_name = json_file.stem.lower().replace(\" \", \"_\")\n",
    "        if collection_name in existing_collections:\n",
    "            print(f\"⏩ Skipping already uploaded collection: {collection_name}\")\n",
    "            continue\n",
    "\n",
    "        process_json_file(json_file)\n",
    "\n",
    "# --- Example usage ---\n",
    "# from upload_chunks_to_qdrant import upload_all_json_chunks\n",
    "# upload_all_json_chunks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9acdabdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Skipping already uploaded collection: spengler\n",
      "⏩ Skipping already uploaded collection: ernst_juenger\n",
      "⏩ Skipping already uploaded collection: byung-chul_han\n",
      "⏩ Skipping already uploaded collection: walter_benjamin\n",
      "⏩ Skipping already uploaded collection: jean_baudrillard\n",
      "⏩ Skipping already uploaded collection: peter_sloterdijk\n",
      "⏩ Skipping already uploaded collection: jacques_ellul\n",
      "⏩ Skipping already uploaded collection: marshall_mcluhan\n",
      "⏩ Skipping already uploaded collection: walter_ong\n",
      "⏩ Skipping already uploaded collection: nick_land\n",
      "⏩ Skipping already uploaded collection: deleuze_and_guattari\n",
      "⏩ Skipping already uploaded collection: carl_schmitt\n",
      "⏩ Skipping already uploaded collection: paul_virilio\n",
      "⏩ Skipping already uploaded collection: vilem_flusser\n",
      "⏩ Skipping already uploaded collection: lewis_mumford\n"
     ]
    }
   ],
   "source": [
    "upload_all_json_chunks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
